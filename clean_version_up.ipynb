{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c7644",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import miceforest as mf\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from deps.utils import *\n",
    "\n",
    "from deps.prevent_equations import *\n",
    "\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from deps.utils import *\n",
    "import shap\n",
    "from deps.personalized_survival import *\n",
    "from deps.cox_regression_modelling import *\n",
    "from deps.extract_KM_curve import *\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from typing import List, Dict, Callable, Union, Tuple\n",
    "import networkx as nx\n",
    "\n",
    "import plotly\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from networkx import Graph, degree, get_edge_attributes\n",
    "from networkx.classes.reportviews import DegreeView\n",
    "from pandas import DataFrame\n",
    "from toolz import identity\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "import networkx\n",
    "from community import community_louvain\n",
    "from random import randint\n",
    "\n",
    "from math import pi\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "from typing import List, Dict, TypedDict, Tuple, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ecaf9",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = \"\" # change this to your path\n",
    "out_path_folder = \"\" # change this to your path\n",
    "\n",
    "results_path = \"\" # change\n",
    "\n",
    "save_results_km = f\"\" # change\n",
    "\n",
    "df_baseline = pd.read_excel(\"\") # change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f53109",
   "metadata": {},
   "source": [
    "# Declarations and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ea29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1000\n",
    "random.seed(SEED)\n",
    "\n",
    "sex_col_name = \"sex_female0\"\n",
    "age_col_name = \"age\"\n",
    "bmi_col_name = 'bmi'\n",
    "male_code = 1 \n",
    "female_code = 0\n",
    "\n",
    "outcomes_defs = [\"acv2\", \"tacv2_y\"]\n",
    "bin_event_var = outcomes_defs[0]\n",
    "time_event_var = outcomes_defs[1]\n",
    "\n",
    "event_out_var = outcomes_defs[0]\n",
    "time_out_var = outcomes_defs[1]\n",
    "\n",
    "proteins_cols = [] # change to list with proteins names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eprime_avg = \"\" # change this to their respective column name \n",
    "e_a_ratio = \"\"\n",
    "e_eprime_ratio = \"\"\n",
    "la_strain = \"\"\n",
    "lavi = \"\"\n",
    "lvmi = \"\" \n",
    "rwt = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76af75a",
   "metadata": {},
   "source": [
    "# A. Dimensionality reduction with supervised machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49137117",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap = True\n",
    "num_mice_datasets = 5\n",
    "outcomes_defs = [] # list with 1) events column and 2) time to event column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa812e7d",
   "metadata": {},
   "source": [
    "## GBSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [50, 100,],   # Number of boosting stages\n",
    "    \"model__learning_rate\": [0.05, 0.1, 0.2],  # Learning rate\n",
    "    \"model__max_depth\": [3, 6],  # Tree depth\n",
    "    \"model__min_samples_leaf\": [2, 5, 10]  # Minimum samples in each leaf\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()), \n",
    "    (\"model\", GradientBoostingSurvivalAnalysis(random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25307cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_event_var = outcomes_defs[0]\n",
    "time_event_var = outcomes_defs[1]\n",
    "\n",
    "df = df_baseline[proteins_cols+[bin_event_var, time_event_var]]\n",
    "\n",
    "y = np.array([(event, time) for event, time in zip(df[bin_event_var], df[time_event_var])], dtype=[(\"event\", bool), (\"time\", float)])\n",
    "\n",
    "X = df[proteins_cols]\n",
    "\n",
    "X = impute_input(X, num_mice_datasets, SEED)\n",
    "\n",
    "### Getting best hyperparameters\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y['event'], random_state=SEED)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=c_index_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "compute_c_metrics(grid_search, X_train, X_test, y_train, y_test)\n",
    "\n",
    "### Rebuild model with all data\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "feature_names = proteins_cols\n",
    "df_X_scaled = pd.DataFrame(X_scaled, columns=proteins_cols)\n",
    "\n",
    "model = GradientBoostingSurvivalAnalysis(learning_rate = grid_search.best_params_[\"model__learning_rate\"],\n",
    "                                        max_depth = grid_search.best_params_[\"model__max_depth\"],\n",
    "                                        n_estimators = grid_search.best_params_[\"model__n_estimators\"],\n",
    "                                        min_samples_leaf = grid_search.best_params_[\"model__min_samples_leaf\"],\n",
    "                                        random_state=SEED\n",
    "                                        ).fit(df_X_scaled, y)\n",
    "\n",
    "# Evaluate best model using Concordance Index\n",
    "c_index = model.score(df_X_scaled, y)\n",
    "print(f\"Optimized Concordance Index (all): {c_index:.3f}\")\n",
    "\n",
    "### Plot SHAP\n",
    "if plot_shap:\n",
    "    # Reobtain model to avoid warnings\n",
    "    model_rawX = GradientBoostingSurvivalAnalysis(learning_rate = grid_search.best_params_[\"model__learning_rate\"],\n",
    "                                        max_depth = grid_search.best_params_[\"model__max_depth\"],\n",
    "                                        n_estimators = grid_search.best_params_[\"model__n_estimators\"],\n",
    "                                        min_samples_leaf = grid_search.best_params_[\"model__min_samples_leaf\"],\n",
    "                                        random_state=SEED\n",
    "                                        ).fit(X_scaled, y)\n",
    "    explainer = shap.Explainer(model_rawX.predict, X_scaled, feature_names=feature_names, seed=SEED)\n",
    "    shaps = explainer(X_scaled)\n",
    "    shap.plots.beeswarm(shaps, max_display=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute SHAP values for feature ranking\n",
    "feature_importance = np.abs(shaps.values).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame for ranking\n",
    "feature_ranking = pd.DataFrame({'Feature': X.columns, 'SHAP Importance': feature_importance})\n",
    "feature_ranking = feature_ranking.sort_values(by='SHAP Importance', ascending=False)\n",
    "\n",
    "upper_75 = feature_ranking[\"SHAP Importance\"].describe().loc['75%']\n",
    "top_feat_shap = feature_ranking[feature_ranking[\"SHAP Importance\"]>upper_75]\n",
    "display(top_feat_shap)\n",
    "top_gbsa = list(top_feat_shap['Feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4965ff",
   "metadata": {},
   "source": [
    "## RSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1128914",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [50, 100],   # Number of boosting stages\n",
    "    \"model__max_depth\": [3, 6],  # Tree depth\n",
    "    \"model__min_samples_leaf\": [2, 5, 10]  # Minimum samples to split a node\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),  # Feature scaling inside each fold\n",
    "    (\"model\", RandomSurvivalForest(random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_baseline[proteins_cols+[bin_event_var, time_event_var]]\n",
    "\n",
    "y = np.array([(event, time) for event, time in zip(df[bin_event_var], df[time_event_var])], dtype=[(\"event\", bool), (\"time\", float)])\n",
    "\n",
    "X = df[proteins_cols]\n",
    "\n",
    "X = impute_input(X, num_mice_datasets, SEED)\n",
    "\n",
    "\n",
    "### Getting best hyperparameters\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y['event'], random_state=SEED)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=c_index_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "compute_c_metrics(grid_search, X_train, X_test, y_train, y_test)\n",
    "\n",
    "### Rebuild model with all data\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "feature_names = proteins_cols\n",
    "df_X_scaled = pd.DataFrame(X_scaled, columns=proteins_cols)\n",
    "\n",
    "model = RandomSurvivalForest(\n",
    "                                        max_depth = grid_search.best_params_[\"model__max_depth\"],\n",
    "                                        n_estimators = grid_search.best_params_[\"model__n_estimators\"],\n",
    "                                        min_samples_leaf = grid_search.best_params_[\"model__min_samples_leaf\"],\n",
    "                                        random_state=SEED\n",
    "                                        ).fit(df_X_scaled, y)\n",
    "\n",
    "# Evaluate best model using Concordance Index\n",
    "c_index = model.score(df_X_scaled, y)\n",
    "print(f\"Optimized Concordance Index (all): {c_index:.3f}\")\n",
    "\n",
    "### Plot SHAP\n",
    "if plot_shap:\n",
    "    # Reobtain model to avoid warnings\n",
    "    model_rawX = RandomSurvivalForest(\n",
    "                                        max_depth = grid_search.best_params_[\"model__max_depth\"],\n",
    "                                        n_estimators = grid_search.best_params_[\"model__n_estimators\"],  \n",
    "                                        min_samples_leaf = grid_search.best_params_[\"model__min_samples_leaf\"],\n",
    "                                        random_state=SEED\n",
    "                                        ).fit(X_scaled, y)\n",
    "    explainer = shap.Explainer(model_rawX.predict, X_scaled, feature_names=feature_names, seed=SEED)\n",
    "    shaps = explainer(X_scaled)\n",
    "    shap.plots.beeswarm(shaps, max_display=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute SHAP values for feature ranking\n",
    "feature_importance = np.abs(shaps.values).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame for ranking\n",
    "feature_ranking = pd.DataFrame({'Feature': X.columns, 'SHAP Importance': feature_importance})\n",
    "feature_ranking = feature_ranking.sort_values(by='SHAP Importance', ascending=False)\n",
    "\n",
    "upper_75 = feature_ranking[\"SHAP Importance\"].describe().loc['75%']\n",
    "top_feat_shap = feature_ranking[feature_ranking[\"SHAP Importance\"]>upper_75]\n",
    "display(top_feat_shap)\n",
    "top_rsf = list(top_feat_shap['Feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09c511",
   "metadata": {},
   "source": [
    "## Output from outcomes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebdd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_features = list(set(top_gbsa).intersection(set(top_rsf)))\n",
    "intersect_features.sort()\n",
    "selected_proteins = intersect_features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f14da",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_interest = proteins_cols.copy()\n",
    "\n",
    "cols_interest_b = cols_interest \n",
    "node_color_set = ['black'] * len(cols_interest)\n",
    "\n",
    "data_biomarkers_set = df_baseline[cols_interest]\n",
    "data_biomarkers_set.columns = cols_interest_b\n",
    "\n",
    "definedNodeColors = False\n",
    "\n",
    "data_biomarkers_set.dropna(how='any')\n",
    "print(len(data_biomarkers_set))\n",
    "\n",
    "POWERS = range(2,3)\n",
    "\n",
    "greaterThan = False\n",
    "for power in POWERS:\n",
    "    nodes = list(data_biomarkers_set.columns)\n",
    "    adjacency_matrix = data_biomarkers_set.corr(method='spearman')\n",
    "    print(nodes)\n",
    "    graph = make_graph_from_adjacency_matrix(pd.DataFrame(adjacency_matrix, columns=nodes, index=nodes).pow(power))\n",
    "    graph_position = nx.spring_layout(graph, seed=10, k=0.04)\n",
    "    if greaterThan:\n",
    "        annot_labels = adjacency_matrix[adjacency_matrix.abs() >= 0.03]\n",
    "        annot_labels.fillna(0, inplace=True)\n",
    "        annot_labels = annot_labels.to_numpy()\n",
    "        annot_labels\n",
    "        graph = make_graph_from_adjacency_matrix(DataFrame(annot_labels, columns=nodes, index=nodes).pow(power))\n",
    "        graph_position = nx.spring_layout(graph, seed=5, k=10.45)\n",
    "    edges, weights = zip(*nx.get_edge_attributes(graph, 'weight').items())\n",
    "    degree_centrality = dict(nx.degree(graph, weight='weight'))\n",
    "    nodes = degree_centrality.keys()\n",
    "    graph_figsize_arg = dict(figsize=(25, 40), layout='constrained')\n",
    "    pyplot.figure(**graph_figsize_arg)\n",
    "    draw_graph(\n",
    "        graph, graph_position, edge_width_scale=20, node_width_scale=100,\n",
    "        box_background='#00000000', \n",
    "        modules_colors= sns.color_palette(\"colorblind\"),\n",
    "        font_color={frozenset(data_biomarkers_set.columns): '#3d3d3d'},\n",
    "        highlight_markers=selected_proteins,\n",
    "        render = ('graph')\n",
    "    )\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d741075",
   "metadata": {},
   "source": [
    "# B. Phenogrouping derivation by unsupervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456654b",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a927f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_baseline[proteins_cols]\n",
    "\n",
    "X_clust = X[selected_proteins]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_clust_scaled = scaler.fit_transform(X_clust)\n",
    "\n",
    "covariance_type = \"diag\"\n",
    "\n",
    "results = find_optimal_number_of_clusters(max_n_clusters=9, path=os.path.join(results_path, \"cvi\"), training_data=X_clust_scaled, seed=SEED, cov_type=covariance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "X_clust = X[selected_proteins]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_clust_scaled = scaler.fit_transform(X_clust)\n",
    "\n",
    "gm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=SEED).fit(X_clust_scaled)\n",
    "\n",
    "df_baseline[\"cluster_n\"] = gm.predict(X_clust_scaled)\n",
    "df_baseline[\"cluster_n\"]\n",
    "\n",
    "# Relabel\n",
    "df_baseline[\"cluster_n_bin\"] = df_baseline[\"cluster_n\"].map({0:1, 1:0, 2:1}) # 0 lowest risk, 2 highest risk\n",
    "cluster_labels = {0:2, 1:1, 2:3}\n",
    "df_baseline[\"cl_n_sorted\"] = df_baseline[\"cluster_n\"].replace(cluster_labels)\n",
    "\n",
    "df_baseline[\"cluster_n_c1\"] = df_baseline[\"cl_n_sorted\"].map({1:0, 2:1, 3:0})\n",
    "df_baseline[\"cluster_n_c2\"] = df_baseline[\"cl_n_sorted\"].map({1:0, 2:0, 3:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1d637",
   "metadata": {},
   "source": [
    "## ClusterX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c606aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spider = df_baseline[selected_proteins +[\"cl_n_sorted\"]]\n",
    "df_spider = df_spider.reset_index(drop=True)\n",
    "\n",
    "scaler_spider = StandardScaler()\n",
    "scaler_spider.fit(df_spider[selected_proteins])\n",
    "X_spider = scaler_spider.transform(df_spider[selected_proteins])\n",
    "X_spider = pd.DataFrame(X_spider, columns=selected_proteins)\n",
    "\n",
    "df_spider = pd.concat([X_spider, df_spider[[\"cl_n_sorted\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5732e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_proteins_person = ['ACE2',\n",
    "                            'BNP',\n",
    "                            'KIM-1',\n",
    "                            'REN',\n",
    "                            'CD40-L',\n",
    "                            'Dkk-1',\n",
    "                            'Gal-9',\n",
    "                            'RAGE',\n",
    "                            'TRAIL-R2',\n",
    "                            'CTSL1',\n",
    "                            'PAPPA',\n",
    "                            'ANGPT1',\n",
    "                            'PGF',\n",
    "                            'MMP-12',\n",
    "                            'AGRP',\n",
    " ] # only to reorder\n",
    "\n",
    "for clus in [1, 2, 3]:\n",
    "    df_clus = df_spider[df_spider[\"cl_n_sorted\"]==clus]\n",
    "    metric_dict = {}\n",
    "    metric_dict['group']=['TBM']\n",
    "    \n",
    "    for prot in selected_proteins_person:\n",
    "        metric_dict[prot] = [float(df_clus[prot].mean())]\n",
    "        \n",
    "    df = pd.DataFrame(metric_dict)\n",
    "    print(df)\n",
    "    \n",
    "    # ------- PART 1: Create background\n",
    "\n",
    "    # number of variable\n",
    "    categories = list(df)[1:]\n",
    "    N = len(categories)\n",
    "\n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    #fig, ax = plt.subplots(figsize=(5, 5), layout=\"constrained\", polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    #print(categories)\n",
    "    # Draw one axe per variable + add labels\n",
    "    plt.tick_params(axis='both', which='major', pad=10, direction='out', grid_linestyle='dotted', labelsize=24)\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([-2, -1, 0, 1, 2,], \n",
    "    [\"-2\", \"-1\", \"0\", \"1\", \"2\"]\n",
    "    , color=\"black\", size=12)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    \n",
    "    #plt.gca().set_yticklabels_size('15')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "    #highlights the reference    \n",
    "    index_reference = 2\n",
    "    plt.gca().get_yticklabels()[index_reference].set_size('14')\n",
    "    plt.gca().get_yticklabels()[index_reference].set_weight('bold')\n",
    "    ax.yaxis.get_gridlines()[index_reference].set_linestyle('-')\n",
    "    ax.yaxis.get_gridlines()[index_reference].set_linewidth(1.9)\n",
    "    ax.yaxis.get_gridlines()[index_reference].set_color('black')\n",
    "    ax.yaxis.get_gridlines()[index_reference].set_alpha(.5)\n",
    "\n",
    "    color_cat1 = 'royalblue' #BP and Kidney\n",
    "    plt.gca().get_xticklabels()[0].set_color(color_cat1)\n",
    "    plt.gca().get_xticklabels()[1].set_color(color_cat1)\n",
    "    plt.gca().get_xticklabels()[2].set_color(color_cat1)\n",
    "    plt.gca().get_xticklabels()[3].set_color(color_cat1)\n",
    "    \n",
    "    color_cat2 = 'black' # inflam and immune\n",
    "    plt.gca().get_xticklabels()[4].set_color(color_cat2)\n",
    "    plt.gca().get_xticklabels()[5].set_color(color_cat2)\n",
    "    plt.gca().get_xticklabels()[6].set_color(color_cat2)\n",
    "    plt.gca().get_xticklabels()[7].set_color(color_cat2)\n",
    "\n",
    "    color_cat3 = 'darkcyan' #apoptosis\n",
    "    plt.gca().get_xticklabels()[8].set_color(color_cat3)\n",
    "    \n",
    "    color_cat4 = 'grey' #autophagy\n",
    "    plt.gca().get_xticklabels()[9].set_color(color_cat4)\n",
    "    \n",
    "    color_cat5 = 'purple' #proteolysis\n",
    "    plt.gca().get_xticklabels()[10].set_color(color_cat5)\n",
    "    \n",
    "    color_cat6 = 'saddlebrown' #angiogenisis\n",
    "    plt.gca().get_xticklabels()[11].set_color(color_cat6)\n",
    "    plt.gca().get_xticklabels()[12].set_color(color_cat6)\n",
    "    \n",
    "    color_cat7 = 'hotpink' #celllular matrix\n",
    "    plt.gca().get_xticklabels()[13].set_color(color_cat7)\n",
    "    \n",
    "    color_cat8 = 'goldenrod' #metabolism\n",
    "    plt.gca().get_xticklabels()[14].set_color(color_cat8)\n",
    "\n",
    "    if clus==1:\n",
    "        values = df.loc[0].drop('group').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, color='darkgreen',\n",
    "        linestyle='dashed', label=\"Cluster 1\")\n",
    "        #ax.fill(angles, values, color='#005C84', alpha=0.1)\n",
    "\n",
    "    elif clus==2:\n",
    "        values = df.loc[0].drop('group').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, color='darkorange',\n",
    "        linestyle='dotted', label=\"Cluster 2\")\n",
    "        #ax.fill(angles, values, color='purple', alpha=0.1)\n",
    "\n",
    "    if clus==3:\n",
    "        values = df.loc[0].drop('group').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, color='darkred',\n",
    "        linestyle='dashdot', label=\"Cluster 3\")\n",
    "        #ax.fill(angles, values, color='#005C84', alpha=0.1)\n",
    "\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(1, 1), fontsize=11)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_clusters = True\n",
    "\n",
    "if shap_clusters:\n",
    "    explainer2 = shap.Explainer(gm.predict_proba, X_clust_scaled, feature_names=intersect_features, seed=SEED)\n",
    "    shaps = explainer2(X_clust_scaled)\n",
    "\n",
    "    labels = gm.predict(X_clust_scaled)\n",
    "\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        shap.summary_plot(shap_values=shaps[:, :, i], features=X_clust_scaled, \n",
    "                        max_display=18,\n",
    "                        feature_names=intersect_features, show=False)\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim(-0.8, 0.8)\n",
    "        ax.set_title(f\"Cluster {cluster_labels[i]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532c5c2",
   "metadata": {},
   "source": [
    "# C1. Phenogroups clinical validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9129a7",
   "metadata": {},
   "source": [
    "## Clinal characteristics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5412ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dict = {\n",
    "    \"age\": [\"mean+-std\", 1, \"Age, y\"],\n",
    "    \"bmi\": [\"mean+-std\", 2, \"Body mass index, kg/m\\u00b2\"],\n",
    "    \"sbp\": [\"mean+-std\", 2,  \"Systolic pressure, mm Hg\"], \n",
    "    \"dbp\": [\"mean+-std\", 2,  \"Diastolic pressure, mm Hg\"], \n",
    "    \"pr\": [\"mean+-std\", 2,  \"Heart rate, beats/min\"], \n",
    "    \"smk_1\": [\"n_bin\", 1, \"Current smoking, n(%)\"],\n",
    "}  #examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f34821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df_baseline.copy()\n",
    "df_aux = avg_impute_characteristics(df_aux, int_dict)\n",
    "\n",
    "df_1 = df_aux[df_aux[\"cl_n_sorted\"]==1]\n",
    "df_2 = df_aux[df_aux[\"cl_n_sorted\"]==2]\n",
    "df_3 = df_aux[df_aux[\"cl_n_sorted\"]==3]\n",
    "\n",
    "descriptive = show_organized_descriptive(\n",
    "    [df_aux\n",
    "     ], int_dict, sex_col_name, female_code, male_code, female_code)\n",
    "\n",
    "display(descriptive)\n",
    "\n",
    "descriptive = show_organized_descriptive(\n",
    "    [df_1, df_2, df_3\n",
    "     ], int_dict, sex_col_name, female_code, male_code, female_code)\n",
    "descriptive.columns = ['Variable', 'Cluster 1', 'Cluster 2', 'Cluster 3']\n",
    "\n",
    "\n",
    "print(mannwhitneyu(df_1['prevent_risk_ascvd'], df_2['prevent_risk_ascvd']))\n",
    "print(mannwhitneyu(df_1['prevent_risk_ascvd'], df_3['prevent_risk_ascvd']))\n",
    "print(mannwhitneyu(df_2['prevent_risk_ascvd'], df_3['prevent_risk_ascvd']))\n",
    "\n",
    "print(mannwhitneyu(df_1['ins'], df_2['ins']))\n",
    "print(mannwhitneyu(df_1['ins'], df_3['ins']))\n",
    "print(mannwhitneyu(df_2['ins'], df_3['ins']))\n",
    "\n",
    "#p_from_tstats(descriptive, columns={0: ['2', '5', '*'], 1: ['4', '5', '^']}, recycle_column=False)\n",
    "descriptive_p = p_from_tstats(descriptive, columns={0: ['Cluster 1', 'Cluster 2', '*'], \n",
    "                                    1: ['Cluster 1', 'Cluster 3', '*'],\n",
    "                                    3: ['Cluster 2', 'Cluster 3', '†']}, \n",
    "                              recycle_column=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5b7a5",
   "metadata": {},
   "source": [
    "## Classical survival analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123f778",
   "metadata": {},
   "source": [
    "### KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colouring = {1: \"darkgreen\", 2: \"darkorange\", 3: \"darkred\"}\n",
    "\n",
    "kaplan_meier_plot(dataset=df_baseline, column=\"cl_n_sorted\", te_column=time_out_var, e_column=event_out_var,\n",
    "                      cluster_colours=colouring, \n",
    "                      path=save_results_km, \n",
    "                      max_time=18.1)\n",
    "\n",
    "outcome_stats(df_baseline, \"cl_n_sorted\", t=time_out_var, e=event_out_var, spath=save_results_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a18a1",
   "metadata": {},
   "source": [
    "### Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3599c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars_base = [\"age\", sex_col_name, \"bmi\", \"sbp\", \"tchol\", \"trt_ht\", 'hcv2', \"hdm\", \"smk\"] \n",
    "\n",
    "dependent_variables = input_vars_base.copy() \n",
    "\n",
    "df_baseline_out = df_baseline[dependent_variables + [\"cl_n_sorted\"] + [time_event_var, event_out_var]].dropna()\n",
    "\n",
    "model = ProportionalHazardRatio(time_to_event_column=time_out_var, covariates=dependent_variables,\n",
    "                                event_column=event_out_var, \n",
    "                                group_column=\"cl_n_sorted\", risk_against=\"cluster\").fit(df_baseline_out)\n",
    "results = model.summary()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf43774",
   "metadata": {},
   "source": [
    "# C2. Phenogroups echocardiographic characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29782875",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2105e3a5",
   "metadata": {},
   "source": [
    "### Echo baseline characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dict = {\n",
    "    \"lvidd\": [\"mean+-std\", 2, \"LV internal diameter, cm\"], \n",
    "    \"lvpwd\": [\"mean+-std\", 2, \"LV posterior wall diameter, cm\"], \n",
    "    \"rwt\": [\"mean+-std\", 2, \"Relative wall thickness\"], \n",
    "    \"lvmi\": [\"mean+-std\", 2, \"LV mass index, g/m^2\"], \n",
    "}  # examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df_baseline.dropna(subset=eprime_avg)\n",
    "df_aux = avg_impute_characteristics(df_aux, int_dict)\n",
    "\n",
    "print(len(df_baseline), len(df_aux))\n",
    "\n",
    "df_1 = df_aux[df_aux[\"cl_n_sorted\"]==1]\n",
    "df_2 = df_aux[df_aux[\"cl_n_sorted\"]==2]\n",
    "df_3 = df_aux[df_aux[\"cl_n_sorted\"]==3]\n",
    "\n",
    "\n",
    "descriptive = show_organized_descriptive(\n",
    "    [df_1, df_2, df_3\n",
    "     ], int_dict, sex_col_name, female_code, male_code, female_code)\n",
    "descriptive.columns = ['Variable', 'Cluster 1', 'Cluster 2', 'Cluster 3',  \n",
    "                       ]\n",
    "descriptive\n",
    "descriptive_p = p_from_tstats(descriptive, columns={0: ['Cluster 1', 'Cluster 2', '*'], \n",
    "                                    1: ['Cluster 1', 'Cluster 3', '*'],\n",
    "                                    3: ['Cluster 2', 'Cluster 3', '†'],\n",
    "\n",
    "}, recycle_column=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea0e78",
   "metadata": {},
   "source": [
    "### LS means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2262fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df_baseline.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c151fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_aux -o df_lsmeans -o df_lsmeans_detailed -o df_lsmeans_pairs\n",
    "\n",
    "library(emmeans) \n",
    "library(readxl)\n",
    "\n",
    "set.seed(0)\n",
    "\n",
    "dat_read <- df_aux\n",
    "dat_read$cl_n_sorted <- as.factor(dat_read$cl_n_sorted)\n",
    "dat_read$sex_female0 <- as.factor(dat_read$sex_female0)\n",
    "dat_read$smk_1 <- as.factor(dat_read$smk_1)\n",
    "dat_read$trt_ht <- as.factor(dat_read$trt_ht)\n",
    "\n",
    "echo_cont = c() # echo variables to compare\n",
    "\n",
    "#create data frame\n",
    "df_lsmeans <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "\n",
    "df_lsmeans_detailed <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "df_lsmeans_pairs <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "\n",
    "#provide column names\n",
    "colnames(df_lsmeans) <- c('echo_var', 'cluster1', 'cluster2', 'cluster3')\n",
    "\n",
    "for (i in 1:length(echo_cont)){\n",
    "  set.seed(0)\n",
    "  #print(i)\n",
    "  echo_var = echo_cont[i]\n",
    "  print(paste('-------------------------------------------------------------' ))\n",
    "  print(paste('         -------------------', echo_var, '-------------------        ' ))\n",
    "  print(paste('-------------------------------------------------------------' ))\n",
    "  \n",
    "  \n",
    "  fit1 = lm(get(echo_var) ~ age + sex_female0 + bmi + pr + sbp + trt_ht + cl_n_sorted, data = dat_read)\n",
    "  \n",
    "  print(summary(fit1))\n",
    "  \n",
    "  res_em = emmeans(fit1, \"cl_n_sorted\"\n",
    "                   )\n",
    "\n",
    "  test_em <- test(res_em)\n",
    "  \n",
    "  comp_em <- pairs(res_em,  \n",
    "                   adjust=\"none\" \n",
    "  )\n",
    "  \n",
    "  res_em <- data.frame(res_em)\n",
    "  print(res_em)\n",
    "  test_em_df <- data.frame(test_em)\n",
    "  print(test_em_df)\n",
    "  comp_em_df <- data.frame(comp_em)\n",
    "  comp_em_df['echo_var'] <- echo_var\n",
    "  print(comp_em_df)\n",
    "  \n",
    "  df_lsmeans[i, 'echo_var'] <- echo_var\n",
    "    \n",
    "  for (ls_i in c(1, 2, 3)){\n",
    "    df_lsmeans[i, paste0('cluster', res_em[ls_i, \"cl_n_sorted\"])] <- paste(round(res_em[ls_i, \"emmean\"], 3), \"±\", round(res_em[ls_i, \"SE\"]*sqrt(nrow(dat_read)),3))\n",
    "  }\n",
    "  \n",
    "  for (ls_i in c(1, 2, 3)){\n",
    "    test_em_df[ls_i, \"Lower\"] <-  res_em[ls_i, \"lower.CL\"]\n",
    "    test_em_df[ls_i, \"Upper\"] <- res_em[ls_i, \"upper.CL\"]\n",
    "    test_em_df[ls_i, \"echo_var\"] <- echo_var\n",
    "  }\n",
    "  \n",
    "  for (ls_i in c(1, 2, 3)){\n",
    "    comp_em_df[ls_i, \"group1\"] <- unlist(strsplit(unlist(strsplit(comp_em_df[ls_i,'contrast'], \" - \"))[1], \"cl_n_sorted\"))[2]\n",
    "    comp_em_df[ls_i, \"group2\"] <- unlist(strsplit(unlist(strsplit(comp_em_df[ls_i,'contrast'], \" - \"))[2], \"cl_n_sorted\"))[2]\n",
    "  }\n",
    "  \n",
    "  df_lsmeans_detailed <- rbind(df_lsmeans_detailed, test_em_df)\n",
    "  df_lsmeans_pairs <- rbind(df_lsmeans_pairs, comp_em_df)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33001ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_echo = set(list(df_lsmeans_pairs[df_lsmeans_pairs[\"p.value\"]<0.05]['echo_var']))\n",
    "df_lsmeans_detailed = df_lsmeans_detailed[df_lsmeans_detailed['echo_var'].isin(list(sig_echo))]\n",
    "\n",
    "i=0\n",
    "\n",
    "fig, geeeks = plt.subplots() \n",
    "#echo_var = 'lvidd'\n",
    "\n",
    "for echo_var in sig_echo:\n",
    "    print(echo_var)\n",
    "\n",
    "    plt.xticks([1, 2, 3], ['Cluster 1', 'Cluster 2', 'Cluster 3'], fontsize=14)\n",
    "    df_group = df_lsmeans_detailed[df_lsmeans_detailed['echo_var']==echo_var]\n",
    "    plot_confidence_interval(1, [df_group.iloc[0, :]['emmean'], df_group.iloc[0, :]['Lower'], df_group.iloc[0, :]['Upper']])\n",
    "    plot_confidence_interval(2, [df_group.iloc[1, :]['emmean'], df_group.iloc[1, :]['Lower'], df_group.iloc[1, :]['Upper']])\n",
    "    plot_confidence_interval(3, [df_group.iloc[2, :]['emmean'], df_group.iloc[2, :]['Lower'], df_group.iloc[2, :]['Upper']])\n",
    "    h = 0.01\n",
    "    ypos = df_group['Upper'].max()\n",
    "    ymin = df_group['Lower'].min()\n",
    "    df_comp = df_lsmeans_pairs[df_lsmeans_pairs['echo_var']==echo_var]\n",
    "    #print(df_comp)\n",
    "    for i in range(3):\n",
    "        if df_comp.iloc[i, :]['p.value'] < 0.05:\n",
    "            x1 = int(df_comp.iloc[i, :]['group1'])\n",
    "            x2 = int(df_comp.iloc[i, :]['group2'])\n",
    "            h = 0.12*(ypos-ymin) if (x1==1 and x2==3)|(x1==3 and x2==1) else 0.05*(ypos-ymin)\n",
    "            p_value = \"P<0.0001\" if df_comp.iloc[i, :]['p.value'] < 0.0001 else f\"P={round(df_comp.iloc[i, :]['p.value'], 4):.4f}\"\n",
    "            #print(p_value)\n",
    "            plt.text(((x1+x2)/2), ypos+h, p_value, ha='center', va='bottom', fontsize=12)\n",
    "            plt.plot([x1, x1, x2, x2], \n",
    "                    [ypos+.2*h, ypos+h, ypos+h, ypos+1.5*h], lw=0.5, color='white')#just to adjust y axis\n",
    "            plt.plot([x1, x1, x2, x2], \n",
    "                    [ypos+.2*h, ypos+h, ypos+h, ypos+.2*h], lw=0.5, color='black')\n",
    "\n",
    "    plt.ylabel(int_dict[echo_var][2], fontsize=15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.gca().yaxis.set_major_locator(ticker.LinearLocator(5))\n",
    "    \n",
    "    if (ypos - ymin) < 0.5:    \n",
    "        plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(fmt_two_digits))\n",
    "    elif (ypos - ymin) < 1.5:    \n",
    "        plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(fmt_one_digit))\n",
    "    else:\n",
    "        plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(fmt_zero_digit))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ff0ea",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d833b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [] #variables to adjust\n",
    "\n",
    "y_var_cat = \"\" #LV remodeling variable\n",
    "\n",
    "df_aux_lvh = df_aux.copy()\n",
    "df_aux_lvh.dropna(subset=x_cols+[y_var_cat], inplace=True)\n",
    "lr_results, _, _ = backward_lr_regression(df_aux_lvh, y_var_cat, x_cols, cte=True, back=True, n_round=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_cat = \"\" #LVDD variable\n",
    "\n",
    "df_aux_lvh = df_aux.copy()\n",
    "df_aux_lvh.dropna(subset=x_cols+[y_var_cat], inplace=True)\n",
    "lr_results, _, _ = backward_lr_regression(df_aux_lvh, y_var_cat, x_cols, cte=True, back=True, n_round=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d00aee",
   "metadata": {},
   "source": [
    "# Longitudinal changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff803c",
   "metadata": {},
   "source": [
    "## Delta and FU computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35656cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fu = pd.read_excel(\"\") #change to FU file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0beb6",
   "metadata": {},
   "source": [
    "## Clinical and echocardiographic characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd52dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dict_v2 = {\n",
    "    \"age\": [\"mean+-std\", 1, \"Age, y\"],\n",
    "    \"bmi\": [\"mean+-std\", 2, \"Body mass index, kg/m\\u00b2\"],\n",
    "    \"sbp\": [\"mean+-std\", 2,  \"Systolic pressure, mm Hg\"], \n",
    "    \"dbp\": [\"mean+-std\", 2,  \"Diastolic pressure, mm Hg\"], \n",
    "    \"pp\": [\"mean+-std\", 2,  \"Pulse pressure, mm Hg\"], \n",
    "} # examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_aux[df_aux[\"cl_n_sorted\"]==1]\n",
    "df_2 = df_aux[df_aux[\"cl_n_sorted\"]==2]\n",
    "df_3 = df_aux[df_aux[\"cl_n_sorted\"]==3]\n",
    "\n",
    "\n",
    "int_dict = int_dict_v2 \n",
    "df_fu_aux = avg_impute_characteristics(df_fu_aux, int_dict)\n",
    "\n",
    "df_1_fu = df_fu_aux.loc[df_aux[df_aux[\"cl_n_sorted\"]==1].index, :]\n",
    "df_2_fu = df_fu_aux.loc[df_aux[df_aux[\"cl_n_sorted\"]==2].index, :]\n",
    "df_3_fu = df_fu_aux.loc[df_aux[df_aux[\"cl_n_sorted\"]==3].index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8008fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive = show_organized_descriptive(\n",
    "    [df_1, df_1_fu,\n",
    "     ], int_dict, sex_col_name, female_code, male_code, female_code)\n",
    "descriptive.columns = ['Variable', 'Baseline', 'Follow-up',  \n",
    "                       ]\n",
    "descriptive_p = p_from_tstats(descriptive, columns={0: ['Baseline', 'Follow-up', '*'],   \n",
    "}, recycle_column=True)\n",
    "descriptive_p_1 = descriptive_p.copy()\n",
    "\n",
    "\n",
    "descriptive = show_organized_descriptive(\n",
    "    [df_2, df_2_fu,\n",
    "     ], int_dict, sex_col_name, female_code, male_code, female_code)\n",
    "descriptive.columns = ['Variable', 'Baseline', 'Follow-up',  \n",
    "                       ]\n",
    "descriptive_p = p_from_tstats(descriptive, columns={0: ['Baseline', 'Follow-up', '*'],      \n",
    "}, recycle_column=True)\n",
    "descriptive_p_2 = descriptive_p.copy()\n",
    "\n",
    "\n",
    "descriptive = show_organized_descriptive(\n",
    "    [df_3, df_3_fu,\n",
    "     ], int_dict, sex_col_name, female_code, male_code, female_code)\n",
    "descriptive.columns = ['Variable', 'Baseline', 'Follow-up',  \n",
    "                       ]\n",
    "descriptive\n",
    "descriptive_p = p_from_tstats(descriptive, columns={0: ['Baseline', 'Follow-up', '*'],     \n",
    "}, recycle_column=True)\n",
    "descriptive_p_3 = descriptive_p.copy()\n",
    "\n",
    "pd.concat([descriptive_p_1, descriptive_p_2[[\"Baseline\", \"Follow-up\"]], descriptive_p_3[[\"Baseline\", \"Follow-up\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c767b3",
   "metadata": {},
   "source": [
    "### LS means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cdc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_aux -o df_lsmeans -o df_lsmeans_detailed -o df_lsmeans_pairs\n",
    "\n",
    "library(emmeans) \n",
    "library(readxl)\n",
    "\n",
    "dat_read <- df_aux\n",
    "dat_read$cl_n_sorted <- as.factor(dat_read$cl_n_sorted)\n",
    "dat_read$lvh_up <- as.factor(dat_read$lvh_up)\n",
    "dat_read$sex_female0 <- as.factor(dat_read$sex_female0)\n",
    "dat_read$smk_1 <- as.factor(dat_read$smk_1)\n",
    "dat_read$trt_ht <- as.factor(dat_read$trt_ht)\n",
    "\n",
    "echo_cont = c() # delta echo variables\n",
    "\n",
    "#create data frame\n",
    "df_lsmeans <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "\n",
    "df_lsmeans_detailed <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "df_lsmeans_pairs <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "\n",
    "#provide column names\n",
    "colnames(df_lsmeans) <- c('echo_var', 'cluster1', 'cluster2', 'cluster3')\n",
    "\n",
    "for (i in 1:length(echo_cont)){\n",
    "  set.seed(0)\n",
    "  #print(i)\n",
    "  echo_var = echo_cont[i]\n",
    "  print(paste('-------------------------------------------------------------' ))\n",
    "  print(paste('         -------------------', echo_var, '-------------------        ' ))\n",
    "  print(paste('-------------------------------------------------------------' ))\n",
    "  echo_var_aux = unlist(strsplit(echo_var, \"d_\"))[2]\n",
    "  \n",
    "  #examples\n",
    "  if (echo_var==\"d_lvidd\"){\n",
    "    fit1 = lm(d_lvidd ~ lvidd + age +  sex_female0 + bmi + d_bmi + pr + d_pr + sbp + d_sbp + started_or_continued_trt_htn + cl_n_sorted, data = dat_read)\n",
    "  } else if (echo_var==\"d_ivrt\"){\n",
    "    fit1 = lm(d_ivrt ~ ivrt + age + sex_female0 + bmi + d_bmi + pr + d_pr + sbp + d_sbp + started_or_continued_trt_htn + cl_n_sorted, data = dat_read)\n",
    "  }\n",
    "  \n",
    "  print(summary(fit1))\n",
    "  \n",
    "  res_em = emmeans(fit1, \"cl_n_sorted\"#, weights='proportional'\n",
    "                   )\n",
    "\n",
    "  test_em <- test(res_em)\n",
    "  \n",
    "  comp_em <- pairs(res_em, adjust=\"none\" \n",
    "  )\n",
    "  \n",
    "\n",
    "  res_em <- data.frame(res_em)\n",
    "  test_em_df <- data.frame(test_em)\n",
    "  comp_em_df <- data.frame(comp_em)\n",
    "  comp_em_df['echo_var'] <- echo_var\n",
    "  \n",
    "  \n",
    "  df_lsmeans[i, 'echo_var'] <- echo_var\n",
    "  \n",
    "  \n",
    "  for (ls_i in c(1, 2, 3)){\n",
    "    df_lsmeans[i, paste0('cluster', res_em[ls_i, \"cl_n_sorted\"])] <- paste(round(res_em[ls_i, \"emmean\"], 3), \"±\", round(res_em[ls_i, \"SE\"]*sqrt(nrow(dat_read)),3))\n",
    "  }\n",
    "  \n",
    "  for (ls_i in c(1, 2, 3)){\n",
    "    test_em_df[ls_i, \"Lower\"] <-  res_em[ls_i, \"lower.CL\"]\n",
    "    test_em_df[ls_i, \"Upper\"] <- res_em[ls_i, \"upper.CL\"]\n",
    "    test_em_df[ls_i, \"echo_var\"] <- echo_var\n",
    "  }\n",
    "  \n",
    "  for (ls_i in c(1, 2, 3)){\n",
    "    comp_em_df[ls_i, \"group1\"] <- unlist(strsplit(unlist(strsplit(comp_em_df[ls_i,'contrast'], \" - \"))[1], \"cl_n_sorted\"))[2]\n",
    "    comp_em_df[ls_i, \"group2\"] <- unlist(strsplit(unlist(strsplit(comp_em_df[ls_i,'contrast'], \" - \"))[2], \"cl_n_sorted\"))[2]\n",
    "  }\n",
    "  \n",
    "  df_lsmeans_detailed <- rbind(df_lsmeans_detailed, test_em_df)\n",
    "  df_lsmeans_pairs <- rbind(df_lsmeans_pairs, comp_em_df)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_echo = set(list(df_lsmeans_pairs[df_lsmeans_pairs[\"p.value\"]<0.05]['echo_var']))\n",
    "df_lsmeans_detailed = df_lsmeans_detailed[df_lsmeans_detailed['echo_var'].isin(list(sig_echo))]\n",
    "\n",
    "i=0\n",
    "\n",
    "fig, geeeks = plt.subplots() \n",
    "\n",
    "for echo_var in sig_echo:\n",
    "    print(echo_var)\n",
    "\n",
    "    plt.xticks([1, 2, 3], ['Cluster 1', 'Cluster 2', 'Cluster 3'], fontsize=14)\n",
    "    #plt.title('Confidence Interval')\n",
    "    df_group = df_lsmeans_detailed[df_lsmeans_detailed['echo_var']==echo_var]\n",
    "    plot_confidence_interval(1, [df_group.iloc[0, :]['emmean'], df_group.iloc[0, :]['Lower'], df_group.iloc[0, :]['Upper']])\n",
    "    plot_confidence_interval(2, [df_group.iloc[1, :]['emmean'], df_group.iloc[1, :]['Lower'], df_group.iloc[1, :]['Upper']])\n",
    "    plot_confidence_interval(3, [df_group.iloc[2, :]['emmean'], df_group.iloc[2, :]['Lower'], df_group.iloc[2, :]['Upper']])\n",
    "    h = 0.01\n",
    "    ypos = df_group['Upper'].max()\n",
    "    ymin = df_group['Lower'].min()\n",
    "    df_comp = df_lsmeans_pairs[df_lsmeans_pairs['echo_var']==echo_var]\n",
    "    #print(df_comp)\n",
    "    for i in range(3):\n",
    "        if df_comp.iloc[i, :]['p.value'] < 0.05:\n",
    "            x1 = int(df_comp.iloc[i, :]['group1'])\n",
    "            x2 = int(df_comp.iloc[i, :]['group2'])\n",
    "            h = 0.12*(ypos-ymin) if (x1==1 and x2==3)|(x1==3 and x2==1) else 0.05*(ypos-ymin)\n",
    "            p_value = \"P<0.0001\" if df_comp.iloc[i, :]['p.value'] < 0.0001 else f\"P={round(df_comp.iloc[i, :]['p.value'], 4):.4f}\"\n",
    "            plt.text(((x1+x2)/2), ypos+h, p_value, ha='center', va='bottom', fontsize=12)\n",
    "            plt.plot([x1, x1, x2, x2], \n",
    "                    [ypos+.2*h, ypos+h, ypos+h, ypos+1.5*h], lw=0.5, color='white')#just to adjust y axis\n",
    "            plt.plot([x1, x1, x2, x2], \n",
    "                    [ypos+.3*h, ypos+h, ypos+h, ypos+.3*h], lw=0.5, color='black')\n",
    "\n",
    "    plt.ylabel(delta_dict[echo_var][2], fontsize=15)\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.gca().yaxis.set_major_locator(ticker.LinearLocator(5))\n",
    "    \n",
    "    if (ypos - ymin) < 0.5:    \n",
    "        plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(fmt_two_digits))\n",
    "    elif (ypos - ymin) < 1.5:    \n",
    "        plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(fmt_one_digit))\n",
    "    else:\n",
    "        plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(fmt_zero_digit))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c3398",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fe879",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [] #input variables\n",
    "\n",
    "y_var_cat = \"d_lv_remol_inc\"\n",
    "\n",
    "df_aux_lvh_up = df_aux[(df_aux[\"d_lv_remol_inc\"]==1)|(df_aux['d_lv_remol_same_down']==1)]\n",
    "lr_results, _, _ = backward_lr_regression(df_aux_lvh_up, y_var_cat, x_cols, cte=True, back=True, n_round=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_cat = \"d_lvdd_inc\"\n",
    "\n",
    "df_aux_lvdd = df_aux[(df_aux[\"d_lvdd_inc\"]==1)|(df_aux['d_lvdd_same_down']==1)]\n",
    "_, _, _ = backward_lr_regression(df_aux_lvdd, y_var_cat, x_cols, cte=True, back=True, n_round=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
